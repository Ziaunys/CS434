\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath} % just math
\usepackage{amssymb} % allow blackboard bold (aka N,R,Q sets)
\usepackage{ulem}
\usepackage{graphicx}
\usepackage{float}
\linespread{1.6}  % double spaces lines
\usepackage[left=1in,top=1in,right=1in,bottom=1in,nohead]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{floatrow}
\usepackage{blindtext}


\begin{document}
\setcounter{subsection}{2} 
\begin{flushright}
\end{flushright}
\begin{flushleft}
\textbf{Eric Zounes} \\
\today \\ 
CS434: Assignment 3
\end{flushleft}

\begin{enumerate} 
	\item[1.] Association Rules. Given the following database of trasactions: \\
	\begin{tabular}{| l | r |}  
		\hline 
		\multicolumn{2}{| l | c |}{} \\
		\hline
		Transaction ID & Items \hline \\
		1 & A,B  \\
		2 & A, B, C, D  \\
		3 & A, D, \\
		4 & C, D \\
		\hline
	\end{tabular}  
	\begin{enumerate} 
		\item[a.] Find all frequent itemsets with minimum support count $= 2$. Pease provide clear intermediate steps, indicating $C_{1},L_{1},C_{2},L_{2}, \ldots$ until the algorithm terminates. Be sure to include (and clearly indicate) the results of the self-join, and pruning step in generating candidate sets. \\
		\item[b.] Find all association rules with miminum support count $= 2$ and min\_conf = $75$.  \\
	\end{enumerate} 
	\item[2.] Please consider the data provided on the class website, which is a 2 dimensional data that comes from a Gaussian distributino with a full covariance matrix. Apply PCA to this data. In particular, you need to first estimate the covariance matrix of this data, and the two PCA projection vectors (i.e., the eigen vectors of the covariance matrix). \\
	\begin{enumerate} 
		\item[a.] Report the covariance matrix that you estimated from the data. \\
				\begin{verbatim} 
					MATLAB code:
						M = csvread('pcs.csv')
						V = cov(M)
						  = 0.9301    0.8121
							0.8121    2.0456
				\end{verbatim} 
		\item[b.] Report the projection vectors for PC1 and PC2. \\
				\begin{verbatim} 	

				  [x1, x2] = eig(V)
						   

				\end{verbatim} 
		\item[c.] Please plot the data in two difference figures, one in the original 2-d coordinate system, and one in the PCA space(2-d). \\
				

	\end{enumerate}
	\item[3.] Given a dataset, PCA seeks to find a small number of dimensions that preserve as much variance in data as possible. Now consider a supervised learning task, can you come up with a situation where applying PCA to the data can significantly worsen the classification performance? In other words, is it possible to have data that is original separable but made non-separable by PCA projection (to lower dimension)? Please provide a visual example \\
	\item[4.] Boosting. Consider a data set $D = {x_{1},x_{2},\ldots,x_{10}}$. We apply Adaboost with Decision Stump. Let $w_{1}, \ldots, w_{10}$ be the weights of the ten training examples respectively. \\
	\begin{enumerate}
		\item[a.] In the first iteration $, x_{1}, x_{2},$and$ x_{3}$ are misclassified. Please rank the updated weights $w_{1}, \ldots, w_{10}$ be the weights of the ten training examples respectively. \\
		\item[b.] In the second iteration, $x_{3}$ and $X_{4}$ are misclassified. Please provide the rank of the updated weights $w_{1}, \ldots, w_{10}$ in increasing order. Explain your ordering. \\
	\end{enumerate}
\end{enumerate}


 
\end{document} 
